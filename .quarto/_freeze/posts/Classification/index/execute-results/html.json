{
  "hash": "7eca3ec21850a223d031b733db5a6523",
  "result": {
    "markdown": "---\ntitle: \"Classification in Machine Learning\"\nauthor: \"Md Shazalal Tushar\"\ndate: \"2023-12-04\"\ncategories: [Machine Learning, Classification]\nimage: \"Iris.jpg\"\n---\n\n**Classification**\n\nA classification algorithm is a machine learning algorithm that categorizes or assigns predefined labels or classes to data based on its features or attributes. It is a Supervised Learning Technique used to classify new observations.\n\nThe Classification algorithm uses labeled input data since it is a supervised learning technique that includes input and output data. The classification procedure (x) converts a discrete output function (y) to an input variable.\n\nNumerous issues can be solved using classification algorithms, such as image recognition, sentiment analysis, medical diagnosis, and spam email detection. The classification algorithm to be used is determined by the nature of the data as well as the specific requirements of the problem at hand, as different algorithms may perform better for different types of data and tasks.\n\nThere are two types of learners in machine learning classification: lazy and eager learners.\n\n**Eager learners** are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time during the training process because of their eagerness to have a better generalization during the training from learning the weights, but they require less time to make predictions.\n\nMost machine learning algorithms are eager learners, and below are some examples:\n\n-   Logistic Regression.\n\n-   Support Vector Machine.\n\n-   Decision Trees.\n\n-   Artificial Neural Networks.\n\n**Lazy learners or instance-based learners**, on the other hand, do not create any model immediately from the training data, and this is where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they search for the nearest neighbor from the whole training data, which makes them very slow during prediction. Some examples of this kind are:\n\n-   K-Nearest Neighbor.\n\n-   Case-based reasoning.\n\nHowever, some algorithms, such asBallTrees andKDTrees, can be used to improve the prediction latency.\n\n**Different Types of Classification Tasks in Machine Learning**\n\nThere are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications.\n\n**Binary Classification**\n\nIn a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled.Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification.\n\n**Multi-class Classification**\n\nThe multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include- Random Forest, Naive Bayes, K-Nearest Neighbors, Gradient Boosting, SVM, Logistic Regression.\n\n**Multi-label Classification**\n\nIn multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification such as- *Multi-label Decision Trees, Multi-label Gradient Boosting, and Multi-label Random Forests*\n\n**Imbalance Classification**\n\nFor the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data.Using conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms.\n\n**Types of Classification in Machine Learning**\n\nThere are seven types of classification in machine learning and all seven models are calledÂ deep learning classification models.\n\n**Logistic Regression**\n\nIn this algorithmic classification, using logistic functions, the possible outcome of a single trial is modelled. The advantage of this logistic regression is that it receive multiple variables and gives a single output variable. It works when a binary classification machine learningvariable is present.\n\n**Naive Bayes**\n\nBayes is the theorem of algorithmic classification for every single feature. Classification and spam filtering work in many real-world documents. Getting the necessary parameters requires a small amount of training and works extremely fast compared to more experienced methods. It is the advantage of naive Bayes. It works only when there is a predictor variable. And this is the disadvantage of Naive Bayes.\n\n**Stochastic Gradient Descent**\n\nIn linear models of algorithm classification, stochastic gradient descent works very easily and efficiently, supporting the function and penalties. It is structured and simple to execute. This is the advantage of stochastic gradient descent. It is hard to scale. Hence, it requires hyper-parameters. This is the disadvantage of stochastic gradient descent.\n\n**K-Nearest Neighbors**\n\nNeighbour's algorithm classification is known as lazy learning. It does not work in a general internal model but simply stores the training data. It has a simple majority vote for each point. Neighbor algorithm classification is easy to implement and contains a large number of training datasets. This is the advantage of having neighbors. The K value is high and needs to be controlled. This is the disadvantage of the neighbor's classification.\n\n**Decision Tree**\n\nThe classes get the attribute of datasets to classify. The decision tree can handle both numerical and categorical datasets in algorithmic classification. It is easy to understand and visualize. This is the advantage of the decision tree. If it is not generalized well, it may create a decision-tree complex. This is the disadvantage of the decision tree algorithm classification.\n\n**Random Forest**\n\nFor overfitting a model and controlling the Meta, the estimator takes the number of various decision trees to improve the classifier in a random forest. Overfitting and the random forest are better classifiers. It is the advantage of a random forest. It has a complement algorithm for classification and is difficult to implement. And this is the disadvantage of random forests.\n\n**Support vector Machine**\n\nSupport vector machine takes the training data as points and spaces them out into categories by clearing the gap in this algorithm's classification. It is high-dimensional and memory-efficient. This is the advantage of a support vector machine. The algorithmic classification is not provided directly, and they are very expensive in five-fold cross-validation. And this is the disadvantage of a support vector machine.\n\nBelow is an example where I have used the iris dataset to predict the species using linear regression model\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# Load the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Create a DataFrame for better visualization\nconf_df = pd.DataFrame(conf_matrix, index=iris.target_names, columns=iris.target_names)\n\n# Specify the plot size\nplt.figure(figsize=(8, 8))\n\n# Create a heatmap using Matplotlib's imshow\nplt.imshow(conf_df, interpolation='nearest', cmap=plt.cm.Greens)\n\n# Add annotations with actual values\nfor i in range(len(iris.target_names)):\n    for j in range(len(iris.target_names)):\n        plt.text(j, i, str(conf_df.iloc[i, j]), ha='center', va='center')\n\nplt.title('Confusion Matrix Heatmap')\nplt.colorbar()\nplt.xticks(np.arange(len(iris.target_names)), iris.target_names, rotation=0)\nplt.yticks(np.arange(len(iris.target_names)), iris.target_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\shaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=674 height=619}\n:::\n:::\n\n\nNow, I will calculate the accuracy of the model-\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Calculating the accuracy of the classification\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of the model is {:.2f}\".format(accuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the model is 97.37\n```\n:::\n:::\n\n\nThe accuracy of the model is 97.37% as it predicted one virginica which is versicolor in reality.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}