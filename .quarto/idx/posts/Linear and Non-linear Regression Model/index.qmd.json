{"title":"Linear and Non-linear Regression Model","markdown":{"yaml":{"title":"Linear and Non-linear Regression Model","author":"Md Shazalal Tushar","date":"2023-11-29","categories":["Machine Learning","Regression Model"],"image":"Clustering.jpg"},"headingText":"List of independent variables","containsRefs":false,"markdown":"\n\nLinear Regression is a supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable. In other words, it predicts the output variables based on the independent input variable.\n\nThere are two types of linear regression models- **Simple and Multiple**.\n\n**Simple Linear Regression**\n\nIn the simple regression model, there is only one independent variable and the model finds the linear relationship of it with the dependent variable. The equation of a Simple Linear Regression Model is-\n\n$$ y= \\beta_0 + \\beta_1 x+ \\epsilon_0 $$\n\nHere, y is the dependent variable, $\\beta_0$ is the intercept, $\\beta_1$ is the regression coefficient of the independent variable x, $\\epsilon_0$ is the error term of the regression model.\n\n**Multiple Linear Regression**\n\nThe difference between simple linear regression and multiple regression model is that multiple regression model has more than one independent variables. The equation of a multiple linear regression model is given below-\\\n\\\n$$ y = \\beta_0+ \\beta_1* x_1+ \\beta_2 * x_2+ \\beta_3 * x_3+.............+\\beta_n * x_n+ \\epsilon $$\n\nHere, y is the dependent or target variable, $\\beta_0$ is the intercept of the regression line in y axis, $\\beta_1,\\beta_2,\\beta_3, \\beta_n$ are the regression co-efficients of independent variables $x_1,x_2, x_3, x_n$. $\\epsilon$ indicates the error term of the regression model.\n\nA linear regression model's main aim is to find the best fit linear line and the optimal values of intercept and coefficients such that the error is minimized. Error is the difference between the actual value and predicted value and the goal is to reduce the difference.\n\n**Assumption of Linear Regression**\n\nThere are six assumption of linear regression-\n\n-   **Linearity:** It states that the dependent variable Y should be linearly related to independent variables. This assumption can be checked by plotting a scatter plot between both variables.\n\n-   **Normality:** The X and Y variables should be normally distributed. Histograms, KDE plots, Q-Q plots can be used to check the Normality assumption.\n\n-   **Homoscedasticity:** The variance of the error terms should be constant i.e. the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\n\n-   **Independent/ No Multicollinearity:** The variables should be independent of each other i.e. no correlation should be there between the independent variables. To check this assumption, we can use correlation matrix or VIF score. If the VIF score is less than 5 then there is no significant correlation present.\n\n-   The error terms should be normally distributed. Q-Q plots and Histograms can be used to check the distribution of error terms.\n\n-   **No Autocorrelation:** The error terms should be independent of each other. Autocorrelation can be tested using the Durbin Watson test. The null hypothesis assumes that there is no autocorrelation. The value of the test lies between 0 to 4. If the value of the test is 2 then there is no autocorrelation.\n\nThe following is an example of Multiple Linear Regression Model of King County, Washington housing data. For the first step, I will identify the correlation between the independent variables while the dependent variable is the housing price. As a first step, I will remove the cells with no values.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nData= pd.read_csv(\"kc_house_data.csv\")\nData.columns\nData.dropna(inplace=True)\n```\n\nTo keep the model simple, I will use only three variables- Number of bedrooms, Number of bathrooms, and square footage of the living room to predict the price of housing. Hence, I will remove all the other columns from the dataframe.\n\n```{python}\ndrop_columns= ['id', 'date','sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode','lat', 'long', 'sqft_living15', 'sqft_lot15']\nData.drop(columns=drop_columns,inplace=True)\nData.head(5)\n```\n\nNow, I will divide the dataset into training set (70%) and test set (30%). Based on the training set, I will develop the regression model and later predict the housing price based on the model.\n\n```{python}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nindependent_var = [\"bedrooms\", \"bathrooms\", \"sqft_living\"]\n\n# Extract independent and dependent variables\nX = Data[independent_var]\ny = Data['price']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model performance\ndef performance_stat(y_test, y_pred):\n  from sklearn.metrics import r2_score\n\n  # Calculate regression metrics\n  r2 = r2_score(y_test, y_pred)\n  print(f'R-squared (R²): {r2:.2f}')\n\n# Call the performance_stat function\nperformance_stat(y_test, y_pred)\n```\n\nHere, the $R^2$ value of 0.52 indicates that about 52% of variability of the housing price can be explained by the three independent variables. Now, I will generate a scatterplot to see the model performance in terms of predicting the actual housing price.\n\n```{python}\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_pred, y_test, color='green', alpha=0.1)\n\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='red', linewidth=2, label='1:1 Line')\n\nplt.xlabel('Predicted Housing Price')\nplt.ylabel('Actual Housing Price')\nplt.title('Linear Regression on King County Housing Dataset')\nplt.legend()\nplt.show()\n```\n\n**Non-Linear Regression Model**\n\nNon-linear regression algorithms are machine learning techniques used to model and predict non-linear relationships between input variables and target variables. These algorithms aim to capture complex patterns and interactions that cannot be effectively represented by a linear model. Here are some popular non-linear regression algorithms. The example of non-linear regression models are-\n\n-   Decision Trees\n\n-   Random Forest\n\n-   Support Vector Regression (SVR)\n\n-   K-Nearest Neighbors (KNN)\n\n-   Artificial Neural Networks (ANN)\n\n-   Gradient Boosting\n\n-   Polynomial Regression\n\n-   AdaBoost Regression\n\n-   Extra Trees Regression\n\n-   Bayesian Ridge Regression\n\n-   Kernel Ridge Regression\n\n**Applying Random Forest to the housing dataset**\n\nNow, I will use Random Forest to predict the housing price based on the number of bedrooms, bathrooms, and square footage of the living room and check which model performed better based on the $R^2$ value.\n\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create and train the Random Forest Regression model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_rf = model.predict(X_test)\n\n# Evaluate the model\nr2_rf = r2_score(y_test, y_pred)\n\nprint(f'R-squared of RF (R²): {r2_rf:.2f}')\n\n```\n\nHere, the $R^2$ value of the Random Forest regression model is also 52% indicating both of the models performed similarly.\n\n```{python}\n# Visualize the results (2D scatter plot for simplicity)\nplt.figure(figsize=(8, 8))\nplt.scatter(y_pred_rf, y_test, color='grey', alpha= 0.1)\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='black', linewidth=2, label='1:1 Line')\nplt.xlabel('Predicted housing price in RF')\nplt.ylabel('Actual housing price')\nplt.legend()\nplt.show()\n\n```\n","srcMarkdownNoYaml":"\n\nLinear Regression is a supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable. In other words, it predicts the output variables based on the independent input variable.\n\nThere are two types of linear regression models- **Simple and Multiple**.\n\n**Simple Linear Regression**\n\nIn the simple regression model, there is only one independent variable and the model finds the linear relationship of it with the dependent variable. The equation of a Simple Linear Regression Model is-\n\n$$ y= \\beta_0 + \\beta_1 x+ \\epsilon_0 $$\n\nHere, y is the dependent variable, $\\beta_0$ is the intercept, $\\beta_1$ is the regression coefficient of the independent variable x, $\\epsilon_0$ is the error term of the regression model.\n\n**Multiple Linear Regression**\n\nThe difference between simple linear regression and multiple regression model is that multiple regression model has more than one independent variables. The equation of a multiple linear regression model is given below-\\\n\\\n$$ y = \\beta_0+ \\beta_1* x_1+ \\beta_2 * x_2+ \\beta_3 * x_3+.............+\\beta_n * x_n+ \\epsilon $$\n\nHere, y is the dependent or target variable, $\\beta_0$ is the intercept of the regression line in y axis, $\\beta_1,\\beta_2,\\beta_3, \\beta_n$ are the regression co-efficients of independent variables $x_1,x_2, x_3, x_n$. $\\epsilon$ indicates the error term of the regression model.\n\nA linear regression model's main aim is to find the best fit linear line and the optimal values of intercept and coefficients such that the error is minimized. Error is the difference between the actual value and predicted value and the goal is to reduce the difference.\n\n**Assumption of Linear Regression**\n\nThere are six assumption of linear regression-\n\n-   **Linearity:** It states that the dependent variable Y should be linearly related to independent variables. This assumption can be checked by plotting a scatter plot between both variables.\n\n-   **Normality:** The X and Y variables should be normally distributed. Histograms, KDE plots, Q-Q plots can be used to check the Normality assumption.\n\n-   **Homoscedasticity:** The variance of the error terms should be constant i.e. the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\n\n-   **Independent/ No Multicollinearity:** The variables should be independent of each other i.e. no correlation should be there between the independent variables. To check this assumption, we can use correlation matrix or VIF score. If the VIF score is less than 5 then there is no significant correlation present.\n\n-   The error terms should be normally distributed. Q-Q plots and Histograms can be used to check the distribution of error terms.\n\n-   **No Autocorrelation:** The error terms should be independent of each other. Autocorrelation can be tested using the Durbin Watson test. The null hypothesis assumes that there is no autocorrelation. The value of the test lies between 0 to 4. If the value of the test is 2 then there is no autocorrelation.\n\nThe following is an example of Multiple Linear Regression Model of King County, Washington housing data. For the first step, I will identify the correlation between the independent variables while the dependent variable is the housing price. As a first step, I will remove the cells with no values.\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nData= pd.read_csv(\"kc_house_data.csv\")\nData.columns\nData.dropna(inplace=True)\n```\n\nTo keep the model simple, I will use only three variables- Number of bedrooms, Number of bathrooms, and square footage of the living room to predict the price of housing. Hence, I will remove all the other columns from the dataframe.\n\n```{python}\ndrop_columns= ['id', 'date','sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode','lat', 'long', 'sqft_living15', 'sqft_lot15']\nData.drop(columns=drop_columns,inplace=True)\nData.head(5)\n```\n\nNow, I will divide the dataset into training set (70%) and test set (30%). Based on the training set, I will develop the regression model and later predict the housing price based on the model.\n\n```{python}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n# List of independent variables\nindependent_var = [\"bedrooms\", \"bathrooms\", \"sqft_living\"]\n\n# Extract independent and dependent variables\nX = Data[independent_var]\ny = Data['price']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model performance\ndef performance_stat(y_test, y_pred):\n  from sklearn.metrics import r2_score\n\n  # Calculate regression metrics\n  r2 = r2_score(y_test, y_pred)\n  print(f'R-squared (R²): {r2:.2f}')\n\n# Call the performance_stat function\nperformance_stat(y_test, y_pred)\n```\n\nHere, the $R^2$ value of 0.52 indicates that about 52% of variability of the housing price can be explained by the three independent variables. Now, I will generate a scatterplot to see the model performance in terms of predicting the actual housing price.\n\n```{python}\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_pred, y_test, color='green', alpha=0.1)\n\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='red', linewidth=2, label='1:1 Line')\n\nplt.xlabel('Predicted Housing Price')\nplt.ylabel('Actual Housing Price')\nplt.title('Linear Regression on King County Housing Dataset')\nplt.legend()\nplt.show()\n```\n\n**Non-Linear Regression Model**\n\nNon-linear regression algorithms are machine learning techniques used to model and predict non-linear relationships between input variables and target variables. These algorithms aim to capture complex patterns and interactions that cannot be effectively represented by a linear model. Here are some popular non-linear regression algorithms. The example of non-linear regression models are-\n\n-   Decision Trees\n\n-   Random Forest\n\n-   Support Vector Regression (SVR)\n\n-   K-Nearest Neighbors (KNN)\n\n-   Artificial Neural Networks (ANN)\n\n-   Gradient Boosting\n\n-   Polynomial Regression\n\n-   AdaBoost Regression\n\n-   Extra Trees Regression\n\n-   Bayesian Ridge Regression\n\n-   Kernel Ridge Regression\n\n**Applying Random Forest to the housing dataset**\n\nNow, I will use Random Forest to predict the housing price based on the number of bedrooms, bathrooms, and square footage of the living room and check which model performed better based on the $R^2$ value.\n\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create and train the Random Forest Regression model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_rf = model.predict(X_test)\n\n# Evaluate the model\nr2_rf = r2_score(y_test, y_pred)\n\nprint(f'R-squared of RF (R²): {r2_rf:.2f}')\n\n```\n\nHere, the $R^2$ value of the Random Forest regression model is also 52% indicating both of the models performed similarly.\n\n```{python}\n# Visualize the results (2D scatter plot for simplicity)\nplt.figure(figsize=(8, 8))\nplt.scatter(y_pred_rf, y_test, color='grey', alpha= 0.1)\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='black', linewidth=2, label='1:1 Line')\nplt.xlabel('Predicted housing price in RF')\nplt.ylabel('Actual housing price')\nplt.legend()\nplt.show()\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Linear and Non-linear Regression Model","author":"Md Shazalal Tushar","date":"2023-11-29","categories":["Machine Learning","Regression Model"],"image":"Clustering.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}