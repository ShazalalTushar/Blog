{"title":"Clustering","markdown":{"yaml":{"title":"Clustering","author":"Md Shazalal Tushar","date":"2023-11-27","categories":["Machine Learning","Clustering"],"image":"Clustering.jpg"},"headingText":"Load Iris dataset","containsRefs":false,"markdown":"\n\nIn this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.\n\n**Clustering**\n\nClustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.\n\n**Clustering Types**\n\nThere are five types of clustering algorithms-\n\n-   Partitioning Based Clustering\n\n-   Hierarchical Clustering\n\n-   Model-Based Clustering\n\n-   Density-Based Clustering\n\n-   Fuzzy Clustering\n\n***Partitioning Based Clustering***\n\nIn this clustering, the algorithm divides the data into **k** number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.\n\n***Hierarchical Clustering***\n\nUnlike Partitioning Based Clustering, it doesn't require pre-defined number of clusters.There are two types of hierarchical clustering-\n\n**Agglomerative:** This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.\n\n**Divisive:** This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.\n\nWhen it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.\n\n***Model Based Clustering***\n\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\n\n***Density Based Clustering***\n\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\n\n**Fuzzy Clustering**\n\nFuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.\n\n**K-Mean Clustering of Iris Dataset**\n\nNow, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\niris = datasets.load_iris()\n\n# Petal width and length\nX = iris.data[:, 2:4]  \n\n# The range of k values to try\nk_values = range(1, 11)\n\n# Calculate inertia for each k value\ninertia_values = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\n# Plot the inertia values\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.show()\n\n```\n\nHere, we can use k=3. For the next step, I am going to use k=3 for clustering the data-\n\n```{python}\n# Choose the number of clusters (you can adjust this based on your needs)\nk = 3\n\n# Apply k-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the results\nplt.figure(figsize=(8, 5))\n\n# Plot the data points with color-coded clusters\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')\n\n# Plot the cluster centers\nplt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')\n\n# Set plot labels and title\nplt.xlabel('Petal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')\n\n# Show legend\nplt.legend()\n\n# Display the plot\nplt.show()\n\n```\n","srcMarkdownNoYaml":"\n\nIn this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.\n\n**Clustering**\n\nClustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.\n\n**Clustering Types**\n\nThere are five types of clustering algorithms-\n\n-   Partitioning Based Clustering\n\n-   Hierarchical Clustering\n\n-   Model-Based Clustering\n\n-   Density-Based Clustering\n\n-   Fuzzy Clustering\n\n***Partitioning Based Clustering***\n\nIn this clustering, the algorithm divides the data into **k** number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.\n\n***Hierarchical Clustering***\n\nUnlike Partitioning Based Clustering, it doesn't require pre-defined number of clusters.There are two types of hierarchical clustering-\n\n**Agglomerative:** This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.\n\n**Divisive:** This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.\n\nWhen it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.\n\n***Model Based Clustering***\n\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\n\n***Density Based Clustering***\n\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\n\n**Fuzzy Clustering**\n\nFuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.\n\n**K-Mean Clustering of Iris Dataset**\n\nNow, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load Iris dataset\niris = datasets.load_iris()\n\n# Petal width and length\nX = iris.data[:, 2:4]  \n\n# The range of k values to try\nk_values = range(1, 11)\n\n# Calculate inertia for each k value\ninertia_values = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\n# Plot the inertia values\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.show()\n\n```\n\nHere, we can use k=3. For the next step, I am going to use k=3 for clustering the data-\n\n```{python}\n# Choose the number of clusters (you can adjust this based on your needs)\nk = 3\n\n# Apply k-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the results\nplt.figure(figsize=(8, 5))\n\n# Plot the data points with color-coded clusters\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')\n\n# Plot the cluster centers\nplt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')\n\n# Set plot labels and title\nplt.xlabel('Petal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')\n\n# Show legend\nplt.legend()\n\n# Display the plot\nplt.show()\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Clustering","author":"Md Shazalal Tushar","date":"2023-11-27","categories":["Machine Learning","Clustering"],"image":"Clustering.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}