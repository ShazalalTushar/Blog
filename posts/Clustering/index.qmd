---
title: "Clustering"
author: "Md Shazalal Tushar"
date: "2023-11-27"
categories: [Machine Learning, Clustering]
image: "Clustering.jpg"
---

In this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.

**Clustering**

Clustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.

**Clustering Types**

There are five types of clustering algorithms-

-   Partitioning Based Clustering

-   Hierarchical Clustering

-   Model-Based Clustering

-   Density-Based Clustering

-   Fuzzy Clustering

***Partitioning Based Clustering***

In this clustering, the algorithm divides the data into **k** number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.

***Hierarchical Clustering***

Unlike Partitioning Based Clustering, it doesn't require pre-defined number of clusters.There are two types of hierarchical clustering-

**Agglomerative:** This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.

**Divisive:** This is a "top-down" approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.

When it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.

***Model Based Clustering***

These clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.

***Density Based Clustering***

These models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.

**Fuzzy Clustering**

Fuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.

**K-Mean Clustering of Iris Dataset**

Now, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import datasets
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


# Load Iris dataset
iris = datasets.load_iris()

# Petal width and length
X = iris.data[:, 2:4]  

# The range of k values to try
k_values = range(1, 11)

# Calculate inertia for each k value
inertia_values = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertia_values.append(kmeans.inertia_)

# Plot the inertia values
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.grid(True)
plt.show()

```

Here, we can use k=3. For the next step, I am going to use k=3 for clustering the data-

```{python}
# Choose the number of clusters (you can adjust this based on your needs)
k = 3

# Apply k-means clustering
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(X)

# Get cluster centers and labels
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Visualize the results
plt.figure(figsize=(8, 5))

# Plot the data points with color-coded clusters
for i in range(k):
    cluster_points = X[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')

# Plot the cluster centers
plt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')

# Set plot labels and title
plt.xlabel('Petal Width (cm)')
plt.ylabel('Petal Length (cm)')
plt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')

# Show legend
plt.legend()

# Display the plot
plt.show()

```
