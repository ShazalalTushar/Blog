{
  "hash": "c61db338027e5312e0930130351a2a02",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection Using Machine Learning\"\nauthor: \"Md Shazalal Tushar\"\ndate: \"2023-12-06\"\ncategories: [Machine Learning, Anomaly Detection]\nimage: \"Anomaly.jpeg\"\n---\n\n# **Anomaly Detection**\n\nAnomaly detection is the process of identifying data points, entities or events that fall outside the normal range. An anomaly is anything that deviates from what is standard or expected. In practice, anomaly detection is often used to detect suspicious events, unexpected opportunities or bad data buried in time series data. A suspicious event might indicate a network breach, fraud, crime, disease or faulty equipment. An unexpected opportunity could involve finding a store, product or salesperson that's performing much better than others and should be investigated for insight into improving the business.\n\n## **Types of Anomalies**\n\nThere are three types of anomalies-\n\n-   **Global**\n\n    When a data point assumes a value that is far outside all the other data point value ranges in the dataset, it can be considered a global anomaly. In other words, it\\'s a rare event. For example, if you receive an average American salary to your bank accounts each month but one day get a million dollars, that would look like a global anomaly to the bank\\'s analytics team.\n\n-   **Contextual**\n\n    When an outlier is called contextual it means that its value doesn\\'t correspond with what we expect to observe for a similar data point in the same context. Contexts are usually temporal, and the same situation observed at different times can be not an outlier. For example, for stores it\\'s quite normal to experience an increase in customers during the holiday season. However, if a sudden boost happens outside of holidays or sales, it can be considered a contextual outlier.\n\n-   **Collective**\n\n    Collective outliers are represented by a subset of data points that deviate from the normal behavior. In general, tech companies tend to grow bigger and bigger. Some companies may decay but it\\'s not a general trend. However, if many companies at once show a decrease in revenue in the same period of time, we can identify a collective outlier.\n\n## **Methods of Anomaly Detection**\n\n**Supervised**\n\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\n\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\n\n**Unsupervised**\n\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\n\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\n\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it\\'s impossible to predict all the anomalies that can occur in the dataset.\n\n**Semi-supervised**\n\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model\\'s predictions more accurate.\n\n**Local outlier factor (LOF)**\n\nLocal outlier factor is probably the most common technique for anomaly detection. This algorithm is based on the concept of the local density. It compares the local density of an object with that of its neighbouring data points. If a data point has a lower density than its neighbours, then it is considered an outlier.\n\n**K-nearest neighbors**\n\nkNN is a supervised ML algorithm often used for classification. When applied to anomaly detection problems, kNN is a useful tool because it allows to easily visualize the data points on the scatterplot and make anomaly detection much more intuitive. Another benefit of kNN is that it works well on both small and large datasets.\n\nInstead of learning \\'normal\\' and \\'abnormal\\' values to solve the classification problem, kNN doesn\\'t perform any actual learning. So when it comes to anomaly detection, kNN works as an unsupervised learning algorithm. A machine learning expert defines a range of normal and abnormal values manually, and the algorithm breaks this representation into classes by itself.\n\n**Support vector machines**\n\nSupport vector machine (SVM) is also a supervised machine learning algorithm often used for classification. SVMs use hyperplanes in multi-dimensional space to divide data points into classes.\n\nSVM is usually applied when there are more than one classes involved in the problem. However, in anomaly detection it\\'s also used for single class problems. The model is trained to learn the \\'norm\\' and can identify whether unfamiliar data belongs to this class or represents an anomaly.\n\n**DBSCAN**\n\nThis is an unsupervised ML algorithm based on the principle of density. DBSCAN is able to uncover clusters in large spatial datasets by looking at the local density of the data points and generally shows good results when used for anomaly detection. The points that do not belong to any cluster get their own class: -1 so they are easy to identify. This algorithm handles outliers well when the data is represented by non-discrete data points.\n\n**Autoencoders**\n\nThis algorithm is based on the use of artificial neural networks that encode the data by compressing it into the lower dimensions. Then, ANNs decode the data to reconstruct the original input. When we reduce the dimensionality, we don\\'t lose the necessary information because the rules have already been identified in the compressed data. Now we can already discover outliers.\n\n**Bayesian networks**\n\nBayesian networks enable ML engineers to discover anomalies even in high-dimensional data. This method is used when the anomalies that we\\'re looking for are more subtle and harder to discover and visualizing them on the plot might not produce the desired results.\n\nBelow is an example where I have used titanic dataset to detect anomalies-\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\n\ntitanic = pd.read_csv('titanic.csv')\n\n# Extract numerical features (you may choose other features based on your requirement)\nnumerical_features = titanic[['Age', 'Fare']]\n\n# Handling missing values (you may need to handle other missing values based on your dataset)\nnumerical_features = numerical_features.dropna()\n\n# Create and fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.01, random_state=42)\nmodel.fit(numerical_features)\n\n# Predict outliers (anomalies)\npredictions = model.predict(numerical_features)\ntitanic['prediction'] = predictions\n\n# Separate normal and anomaly points\nnormal_points = numerical_features[titanic['prediction'] == 1]\nanomaly_points = numerical_features[titanic['prediction'] == -1]\n\n# Visualize the results\nplt.scatter(normal_points['Age'], normal_points['Fare'], c='blue', label='Normal', alpha=0.7)\nplt.scatter(anomaly_points['Age'], anomaly_points['Fare'], c='red', label='Anomaly', alpha=0.7)\n\nplt.title('Isolation Forest Anomaly Detection on Titanic Dataset')\nplt.xlabel('Age')\nplt.ylabel('Fare')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=593 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}