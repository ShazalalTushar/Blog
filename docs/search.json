[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/First-Post/index.html",
    "href": "posts/First-Post/index.html",
    "title": "Random Variable",
    "section": "",
    "text": "In this blog, I will cover random variable, types of random variable, probability density function (PDF), cumulative density function (CDF), relation between PDF and CDF. At the end, I will also plot PDF and CDF for Capital Bikeshare trip duration under 60 minutes for both type of user- Member and Casual.\nRandom Variable\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon in a sample space. For example, if we roll two fair dice at a time the probability of having a sum of 2 {P(X=2)} is 1/36 since it can only occur when there is 1 on both dices. Similarly the probability of having a sum of 6 {P(X=6)} is 5/36 since we can have a sum of 6 in the following events- {1,5}, {2,4}, {3,3}, {4,2}, {5,1} and the total number of possible events is 36.\nTypes of Random Variable\nThere are two types of random variable-\ni) Discrete Random Variable\nDiscrete Random Variables can have a finite number of distinct values. For example, the number of students present in a class, the number obtained from a dice throw, etc. The probability function for Discrete Random Variable is PMF (Probability Mass Function).\nii) Continuous Random Variable\nContinuous Random Variables can take on an infinite number of values such as the weight of the students in class room. The probability function for continuous random variable is PDF.\nProbability Distributions\nProbabilities assigned to various outcomes in the sample space S, in turn, determine probabilities associated with the values of any particular random variable defined on S.\nProbability Mass Function (PMF) is used for discrete random variables and it describes how the total probability is distributed among all the possible range values of the Random Variable X. For example, the probability mass function of rolling a dice is as follows-\n\n\n\nx\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\np(x)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n\n\n\nProbability Density Function (PDF) is used for continuous random variables and it defines the probability function representing the density of a continuous random variable lying between a specific range of values. PDF satisfies the following-\ni. f(x) \\(\\geq\\) 0, for all x \\(\\epsilon\\) R\nii. f is piecewise continuous\niii. \\(\\int_{-\\infty}^{\\infty}\\) f(x) = 1\nFor a given limit of a to b, the PDF of a continuous random variable will be, \\[P(a\\leq x \\leq b)= \\int_{a}^{b} f(x) dx\\]\nCumulative Density Function (CDF)\nThe Cumulative Distribution Function (CDF), of a real-valued random variable X, evaluated at x, is the probability function that X will take a value less than or equal to x. It is used to describe the probability distribution of random variables. The properties of a CDF is-\n\nEvery CDF \\(F_x\\) is non decreasing and right continuous \\(\\displaystyle \\lim_{x \\to -\\infty} F_x(x) = 0\\) and \\(\\displaystyle \\lim_{x \\to \\infty} F_x(x) = 1\\)\nFor all real numbers a and b with continuous random variable X, then the function fx is equal to the derivative of \\(F_x\\), such that \\[F_x(b) - F_x(a)= P(a&lt;X\\leq b)= \\int_{a}^{b} f_x(x) dx\\]\n\nRelationship between PDF and CDF for a Continuous Random Variable\nLet X be a continuous random variable with pdf f and cdf F.\ni. We can find the CDF by integrating the PDF-\n\\[\nF_x(x)= \\int_{-\\infty}^{\\infty} f(x) dx\n\\]\nii. We can find the PDF by differentiating the CDF-\n\\[f(x)= \\frac{d}{dx} F_x(x)\\]\nExample of PDF and CDF\nFollowing is an example of PDF and CDF of Capital Bikeshare trip duration under 60 minutes for all bikeshare trips at May 2019.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nCabi= pd.read_csv(r'D:\\VT Class Resourse\\2-1\\Machine Learning\\Data for Blog\\201905-capitalbikeshare-tripdata.csv')\nCabi['Duration (minutes)']=Cabi['Duration']/60\ncounts, bin_edges = np.histogram(Cabi['Duration (minutes)'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf, label= 'PDF');\nplt.plot(bin_edges[1:], cdf, label= 'CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1a583e4ab50&gt;\n\n\n\n\n\nFollowing is the PDF and CDF of bikeshare trip duration by Casual user and Regular members-\n\nCasual= Cabi[Cabi['Member type'] == 'Casual']\nMember= Cabi[Cabi['Member type'] == 'Member']\nC_counts, C_bin_edges = np.histogram(Casual['Duration (minutes)'], bins=10,\n                                 density = True)\nC_pdf = C_counts/(sum(C_counts))\nC_cdf = np.cumsum(C_pdf)\nplt.plot(C_bin_edges[1:],C_pdf,label= 'Casual User PDF');\nplt.plot(C_bin_edges[1:], C_cdf,label= 'Casual User CDF')\n\nM_counts, M_bin_edges = np.histogram(Member['Duration (minutes)'], bins=10,\n                                 density = True)\nM_pdf = M_counts/(sum(M_counts))\nM_cdf = np.cumsum(M_pdf)\nplt.plot(M_bin_edges[1:],M_pdf,label= 'Member PDF');\nplt.plot(M_bin_edges[1:], M_cdf,label= 'Member CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1a583e96d10&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variable\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "In this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.\nClustering\nClustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.\nClustering Types\nThere are five types of clustering algorithms-\n\nPartitioning Based Clustering\nHierarchical Clustering\nModel-Based Clustering\nDensity-Based Clustering\nFuzzy Clustering\n\nPartitioning Based Clustering\nIn this clustering, the algorithm divides the data into k number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.\nHierarchical Clustering\nUnlike Partitioning Based Clustering, it doesn’t require pre-defined number of clusters.There are two types of hierarchical clustering-\nAgglomerative: This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.\nDivisive: This is a “top-down” approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.\nWhen it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.\nModel Based Clustering\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\nDensity Based Clustering\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\nFuzzy Clustering\nFuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.\nK-Mean Clustering of Iris Dataset\nNow, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load Iris dataset\niris = datasets.load_iris()\n\n# Petal width and length\nX = iris.data[:, 2:4]  \n\n# The range of k values to try\nk_values = range(1, 11)\n\n# Calculate inertia for each k value\ninertia_values = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\n# Plot the inertia values\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.show()\n\n\n\n\nHere, we can use k=3. For the next step, I am going to use k=3 for clustering the data-\n\n# Choose the number of clusters (you can adjust this based on your needs)\nk = 3\n\n# Apply k-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the results\nplt.figure(figsize=(8, 5))\n\n# Plot the data points with color-coded clusters\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')\n\n# Plot the cluster centers\nplt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')\n\n# Set plot labels and title\nplt.xlabel('Petal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')\n\n# Show legend\nplt.legend()\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]