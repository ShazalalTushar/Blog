[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Linear and Non-linear Regression Model/index.html",
    "href": "posts/Linear and Non-linear Regression Model/index.html",
    "title": "Linear and Non-linear Regression Model",
    "section": "",
    "text": "Linear Regression is a supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable. In other words, it predicts the output variables based on the independent input variable.\nThere are two types of linear regression models- Simple and Multiple.\nSimple Linear Regression\nIn the simple regression model, there is only one independent variable and the model finds the linear relationship of it with the dependent variable. The equation of a Simple Linear Regression Model is-\n\\[ y= \\beta_0 + \\beta_1 x+ \\epsilon_0 \\]\nHere, y is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the regression coefficient of the independent variable x, \\(\\epsilon_0\\) is the error term of the regression model.\nMultiple Linear Regression\nThe difference between simple linear regression and multiple regression model is that multiple regression model has more than one independent variables. The equation of a multiple linear regression model is given below-\n\n\\[ y = \\beta_0+ \\beta_1* x_1+ \\beta_2 * x_2+ \\beta_3 * x_3+.............+\\beta_n * x_n+ \\epsilon \\]\nHere, y is the dependent or target variable, \\(\\beta_0\\) is the intercept of the regression line in y axis, \\(\\beta_1,\\beta_2,\\beta_3, \\beta_n\\) are the regression co-efficients of independent variables \\(x_1,x_2, x_3, x_n\\). \\(\\epsilon\\) indicates the error term of the regression model.\nA linear regression model’s main aim is to find the best fit linear line and the optimal values of intercept and coefficients such that the error is minimized. Error is the difference between the actual value and predicted value and the goal is to reduce the difference.\nAssumption of Linear Regression\nThere are six assumption of linear regression-\n\nLinearity: It states that the dependent variable Y should be linearly related to independent variables. This assumption can be checked by plotting a scatter plot between both variables.\nNormality: The X and Y variables should be normally distributed. Histograms, KDE plots, Q-Q plots can be used to check the Normality assumption.\nHomoscedasticity: The variance of the error terms should be constant i.e. the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\nIndependent/ No Multicollinearity: The variables should be independent of each other i.e. no correlation should be there between the independent variables. To check this assumption, we can use correlation matrix or VIF score. If the VIF score is less than 5 then there is no significant correlation present.\nThe error terms should be normally distributed. Q-Q plots and Histograms can be used to check the distribution of error terms.\nNo Autocorrelation: The error terms should be independent of each other. Autocorrelation can be tested using the Durbin Watson test. The null hypothesis assumes that there is no autocorrelation. The value of the test lies between 0 to 4. If the value of the test is 2 then there is no autocorrelation.\n\nThe following is an example of Multiple Linear Regression Model of King County, Washington housing data. For the first step, I will identify the correlation between the independent variables while the dependent variable is the housing price. As a first step, I will remove the cells with no values.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nData= pd.read_csv(\"kc_house_data.csv\")\nData.columns\nData.dropna(inplace=True)\n\nTo keep the model simple, I will use only three variables- Number of bedrooms, Number of bathrooms, and square footage of the living room to predict the price of housing. Hence, I will remove all the other columns from the dataframe.\n\ndrop_columns= ['id', 'date','sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode','lat', 'long', 'sqft_living15', 'sqft_lot15']\nData.drop(columns=drop_columns,inplace=True)\nData.head(5)\n\n\n\n\n\n\n\n\nprice\nbedrooms\nbathrooms\nsqft_living\n\n\n\n\n0\n221900.0\n3\n1.00\n1180\n\n\n1\n538000.0\n3\n2.25\n2570\n\n\n2\n180000.0\n2\n1.00\n770\n\n\n3\n604000.0\n4\n3.00\n1960\n\n\n4\n510000.0\n3\n2.00\n1680\n\n\n\n\n\n\n\nNow, I will divide the dataset into training set (70%) and test set (30%). Based on the training set, I will develop the regression model and later predict the housing price based on the model.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n# List of independent variables\nindependent_var = [\"bedrooms\", \"bathrooms\", \"sqft_living\"]\n\n# Extract independent and dependent variables\nX = Data[independent_var]\ny = Data['price']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model performance\ndef performance_stat(y_test, y_pred):\n  from sklearn.metrics import r2_score\n\n  # Calculate regression metrics\n  r2 = r2_score(y_test, y_pred)\n  print(f'R-squared (R²): {r2:.2f}')\n\n# Call the performance_stat function\nperformance_stat(y_test, y_pred)\n\nR-squared (R²): 0.52\n\n\nHere, the \\(R^2\\) value of 0.52 indicates that about 52% of variability of the housing price can be explained by the three independent variables. Now, I will generate a scatterplot to see the model performance in terms of predicting the actual housing price.\n\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_pred, y_test, color='green', alpha=0.1)\n\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='red', linewidth=2, label='1:1 Line')\n\nplt.xlabel('Predicted Housing Price')\nplt.ylabel('Actual Housing Price')\nplt.title('Linear Regression on King County Housing Dataset')\nplt.legend()\nplt.show()\n\n\n\n\nNon-Linear Regression Model\nNon-linear regression algorithms are machine learning techniques used to model and predict non-linear relationships between input variables and target variables. These algorithms aim to capture complex patterns and interactions that cannot be effectively represented by a linear model. Here are some popular non-linear regression algorithms. The example of non-linear regression models are-\n\nDecision Trees\nRandom Forest\nSupport Vector Regression (SVR)\nK-Nearest Neighbors (KNN)\nArtificial Neural Networks (ANN)\nGradient Boosting\nPolynomial Regression\nAdaBoost Regression\nExtra Trees Regression\nBayesian Ridge Regression\nKernel Ridge Regression\n\nApplying Random Forest to the housing dataset\nNow, I will use Random Forest to predict the housing price based on the number of bedrooms, bathrooms, and square footage of the living room and check which model performed better based on the \\(R^2\\) value.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create and train the Random Forest Regression model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_rf = model.predict(X_test)\n\n# Evaluate the model\nr2_rf = r2_score(y_test, y_pred)\n\nprint(f'R-squared of RF (R²): {r2_rf:.2f}')\n\nR-squared of RF (R²): 0.52\n\n\nHere, the \\(R^2\\) value of the Random Forest regression model is also 52% indicating both of the models performed similarly.\n\n# Visualize the results (2D scatter plot for simplicity)\nplt.figure(figsize=(8, 8))\nplt.scatter(y_pred_rf, y_test, color='grey', alpha= 0.1)\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='black', linewidth=2, label='1:1 Line')\nplt.xlabel('Predicted housing price in RF')\nplt.ylabel('Actual housing price')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "In this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.\nClustering\nClustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.\nClustering Types\nThere are five types of clustering algorithms-\n\nPartitioning Based Clustering\nHierarchical Clustering\nModel-Based Clustering\nDensity-Based Clustering\nFuzzy Clustering\n\nPartitioning Based Clustering\nIn this clustering, the algorithm divides the data into k number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.\nHierarchical Clustering\nUnlike Partitioning Based Clustering, it doesn’t require pre-defined number of clusters.There are two types of hierarchical clustering-\nAgglomerative: This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.\nDivisive: This is a “top-down” approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.\nWhen it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.\nModel Based Clustering\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\nDensity Based Clustering\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\nFuzzy Clustering\nFuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.\nK-Mean Clustering of Iris Dataset\nNow, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load Iris dataset\niris = datasets.load_iris()\n\n# Petal width and length\nX = iris.data[:, 2:4]  \n\n# The range of k values to try\nk_values = range(1, 11)\n\n# Calculate inertia for each k value\ninertia_values = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\n# Plot the inertia values\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.show()\n\n\n\n\nHere, we can use k=3. For the next step, I am going to use k=3 for clustering the data-\n\n# Choose the number of clusters (you can adjust this based on your needs)\nk = 3\n\n# Apply k-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the results\nplt.figure(figsize=(8, 5))\n\n# Plot the data points with color-coded clusters\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')\n\n# Plot the cluster centers\nplt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')\n\n# Set plot labels and title\nplt.xlabel('Petal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')\n\n# Show legend\nplt.legend()\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Linear and Non-linear Regression Model\n\n\n\n\n\n\n\nMachine Learning\n\n\nRegression Model\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variable\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/First-Post/index.html",
    "href": "posts/First-Post/index.html",
    "title": "Random Variable",
    "section": "",
    "text": "In this blog, I will cover random variable, types of random variable, probability density function (PDF), cumulative density function (CDF), relation between PDF and CDF. At the end, I will also plot PDF and CDF for Capital Bikeshare trip duration under 60 minutes for both type of user- Member and Casual.\nRandom Variable\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon in a sample space. For example, if we roll two fair dice at a time the probability of having a sum of 2 {P(X=2)} is 1/36 since it can only occur when there is 1 on both dices. Similarly the probability of having a sum of 6 {P(X=6)} is 5/36 since we can have a sum of 6 in the following events- {1,5}, {2,4}, {3,3}, {4,2}, {5,1} and the total number of possible events is 36.\nTypes of Random Variable\nThere are two types of random variable-\ni) Discrete Random Variable\nDiscrete Random Variables can have a finite number of distinct values. For example, the number of students present in a class, the number obtained from a dice throw, etc. The probability function for Discrete Random Variable is PMF (Probability Mass Function).\nii) Continuous Random Variable\nContinuous Random Variables can take on an infinite number of values such as the weight of the students in class room. The probability function for continuous random variable is PDF.\nProbability Distributions\nProbabilities assigned to various outcomes in the sample space S, in turn, determine probabilities associated with the values of any particular random variable defined on S.\nProbability Mass Function (PMF) is used for discrete random variables and it describes how the total probability is distributed among all the possible range values of the Random Variable X. For example, the probability mass function of rolling a dice is as follows-\n\n\n\nx\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\np(x)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n\n\n\nProbability Density Function (PDF) is used for continuous random variables and it defines the probability function representing the density of a continuous random variable lying between a specific range of values. PDF satisfies the following-\ni. f(x) \\(\\geq\\) 0, for all x \\(\\epsilon\\) R\nii. f is piecewise continuous\niii. \\(\\int_{-\\infty}^{\\infty}\\) f(x) = 1\nFor a given limit of a to b, the PDF of a continuous random variable will be, \\[P(a\\leq x \\leq b)= \\int_{a}^{b} f(x) dx\\]\nCumulative Density Function (CDF)\nThe Cumulative Distribution Function (CDF), of a real-valued random variable X, evaluated at x, is the probability function that X will take a value less than or equal to x. It is used to describe the probability distribution of random variables. The properties of a CDF is-\n\nEvery CDF \\(F_x\\) is non decreasing and right continuous \\(\\displaystyle \\lim_{x \\to -\\infty} F_x(x) = 0\\) and \\(\\displaystyle \\lim_{x \\to \\infty} F_x(x) = 1\\)\nFor all real numbers a and b with continuous random variable X, then the function fx is equal to the derivative of \\(F_x\\), such that \\[F_x(b) - F_x(a)= P(a&lt;X\\leq b)= \\int_{a}^{b} f_x(x) dx\\]\n\nRelationship between PDF and CDF for a Continuous Random Variable\nLet X be a continuous random variable with pdf f and cdf F.\ni. We can find the CDF by integrating the PDF-\n\\[\nF_x(x)= \\int_{-\\infty}^{\\infty} f(x) dx\n\\]\nii. We can find the PDF by differentiating the CDF-\n\\[f(x)= \\frac{d}{dx} F_x(x)\\]\nExample of PDF and CDF\nFollowing is an example of PDF and CDF of Capital Bikeshare trip duration under 60 minutes for all bikeshare trips at May 2019.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nCabi= pd.read_csv(r'D:\\VT Class Resourse\\2-1\\Machine Learning\\Data for Blog\\201905-capitalbikeshare-tripdata.csv')\nCabi['Duration (minutes)']=Cabi['Duration']/60\ncounts, bin_edges = np.histogram(Cabi['Duration (minutes)'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf, label= 'PDF');\nplt.plot(bin_edges[1:], cdf, label= 'CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1a583e4ab50&gt;\n\n\n\n\n\nFollowing is the PDF and CDF of bikeshare trip duration by Casual user and Regular members-\n\nCasual= Cabi[Cabi['Member type'] == 'Casual']\nMember= Cabi[Cabi['Member type'] == 'Member']\nC_counts, C_bin_edges = np.histogram(Casual['Duration (minutes)'], bins=10,\n                                 density = True)\nC_pdf = C_counts/(sum(C_counts))\nC_cdf = np.cumsum(C_pdf)\nplt.plot(C_bin_edges[1:],C_pdf,label= 'Casual User PDF');\nplt.plot(C_bin_edges[1:], C_cdf,label= 'Casual User CDF')\n\nM_counts, M_bin_edges = np.histogram(Member['Duration (minutes)'], bins=10,\n                                 density = True)\nM_pdf = M_counts/(sum(M_counts))\nM_cdf = np.cumsum(M_pdf)\nplt.plot(M_bin_edges[1:],M_pdf,label= 'Member PDF');\nplt.plot(M_bin_edges[1:], M_cdf,label= 'Member CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1a583e96d10&gt;"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]