[
  {
    "objectID": "posts/Random Variable/index.html",
    "href": "posts/Random Variable/index.html",
    "title": "Random Variable",
    "section": "",
    "text": "In this blog, I will cover random variable, types of random variable, probability density function (PDF), cumulative density function (CDF), relation between PDF and CDF. At the end, I will also plot PDF and CDF for Capital Bikeshare trip duration under 60 minutes for both type of user- Member and Casual.\nRandom Variable\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon in a sample space. For example, if we roll two fair dice at a time the probability of having a sum of 2 {P(X=2)} is 1/36 since it can only occur when there is 1 on both dices. Similarly the probability of having a sum of 6 {P(X=6)} is 5/36 since we can have a sum of 6 in the following events- {1,5}, {2,4}, {3,3}, {4,2}, {5,1} and the total number of possible events is 36.\nTypes of Random Variable\nThere are two types of random variable-\ni) Discrete Random Variable\nDiscrete Random Variables can have a finite number of distinct values. For example, the number of students present in a class, the number obtained from a dice throw, etc. The probability function for Discrete Random Variable is PMF (Probability Mass Function).\nii) Continuous Random Variable\nContinuous Random Variables can take on an infinite number of values such as the weight of the students in class room. The probability function for continuous random variable is PDF.\nProbability Distributions\nProbabilities assigned to various outcomes in the sample space S, in turn, determine probabilities associated with the values of any particular random variable defined on S.\nProbability Mass Function (PMF) is used for discrete random variables and it describes how the total probability is distributed among all the possible range values of the Random Variable X. For example, the probability mass function of rolling a dice is as follows-\n\n\n\nx\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\np(x)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n\n\n\nProbability Density Function (PDF) is used for continuous random variables and it defines the probability function representing the density of a continuous random variable lying between a specific range of values. PDF satisfies the following-\ni. f(x) \\(\\geq\\) 0, for all x \\(\\epsilon\\) R\nii. f is piecewise continuous\niii. \\(\\int_{-\\infty}^{\\infty}\\) f(x) = 1\nFor a given limit of a to b, the PDF of a continuous random variable will be, \\[P(a\\leq x \\leq b)= \\int_{a}^{b} f(x) dx\\]\nCumulative Density Function (CDF)\nThe Cumulative Distribution Function (CDF), of a real-valued random variable X, evaluated at x, is the probability function that X will take a value less than or equal to x. It is used to describe the probability distribution of random variables. The properties of a CDF is-\n\nEvery CDF \\(F_x\\) is non decreasing and right continuous \\(\\displaystyle \\lim_{x \\to -\\infty} F_x(x) = 0\\) and \\(\\displaystyle \\lim_{x \\to \\infty} F_x(x) = 1\\)\nFor all real numbers a and b with continuous random variable X, then the function fx is equal to the derivative of \\(F_x\\), such that \\[F_x(b) - F_x(a)= P(a&lt;X\\leq b)= \\int_{a}^{b} f_x(x) dx\\]\n\nRelationship between PDF and CDF for a Continuous Random Variable\nLet X be a continuous random variable with pdf f and cdf F.\ni. We can find the CDF by integrating the PDF-\n\\[\nF_x(x)= \\int_{-\\infty}^{\\infty} f(x) dx\n\\]\nii. We can find the PDF by differentiating the CDF-\n\\[f(x)= \\frac{d}{dx} F_x(x)\\]\nExample of PDF and CDF\nFollowing is an example of PDF and CDF of Capital Bikeshare trip duration under 60 minutes for all bikeshare trips at May 2019.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nCabi= pd.read_csv(r'D:\\VT Class Resourse\\2-1\\Machine Learning\\Data for Blog\\201905-capitalbikeshare-tripdata.csv')\nCabi['Duration (minutes)']=Cabi['Duration']/60\ncounts, bin_edges = np.histogram(Cabi['Duration (minutes)'], bins=10,\n                                 density = True)\npdf = counts/(sum(counts))\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf, label= 'PDF');\nplt.plot(bin_edges[1:], cdf, label= 'CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2904dff2b50&gt;\n\n\n\n\n\nFollowing is the PDF and CDF of bikeshare trip duration by Casual user and Regular members-\n\nCasual= Cabi[Cabi['Member type'] == 'Casual']\nMember= Cabi[Cabi['Member type'] == 'Member']\nC_counts, C_bin_edges = np.histogram(Casual['Duration (minutes)'], bins=10,\n                                 density = True)\nC_pdf = C_counts/(sum(C_counts))\nC_cdf = np.cumsum(C_pdf)\nplt.plot(C_bin_edges[1:],C_pdf,label= 'Casual User PDF');\nplt.plot(C_bin_edges[1:], C_cdf,label= 'Casual User CDF')\n\nM_counts, M_bin_edges = np.histogram(Member['Duration (minutes)'], bins=10,\n                                 density = True)\nM_pdf = M_counts/(sum(M_counts))\nM_cdf = np.cumsum(M_pdf)\nplt.plot(M_bin_edges[1:],M_pdf,label= 'Member PDF');\nplt.plot(M_bin_edges[1:], M_cdf,label= 'Member CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2904e040590&gt;"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "In this blog, I will cover clustering, types of clustering, and an example of clustering using XYZ data.\nClustering\nClustering is the most popular technique in unsupervised learning where data is grouped based on the similarity of the data points. The basic principle behind clustering is the assignment of a given set of observations into subgroups or clusters so that observations in the same cluster are somewhat similar. It is a method of unsupervised learning as there is no label attached to the object and the machine has to identify the patterns itself without any input-output mapping. The algorithm is able to extract inferences from the nature of data objects and then create distinct classes to group them appropriately.\nClustering Types\nThere are five types of clustering algorithms-\n\nPartitioning Based Clustering\nHierarchical Clustering\nModel-Based Clustering\nDensity-Based Clustering\nFuzzy Clustering\n\nPartitioning Based Clustering\nIn this clustering, the algorithm divides the data into k number of pre-defined groups. Example of Partitioning Based Clustering includes- K means clustering.\nHierarchical Clustering\nUnlike Partitioning Based Clustering, it doesn’t require pre-defined number of clusters.There are two types of hierarchical clustering-\nAgglomerative: This is a bottom-up approach where each observation is treated as its own cluster in the beginning and as we move from bottom to top, each observation is merged into pairs, and pairs are merged into clusters.\nDivisive: This is a “top-down” approach: all observations start in one cluster, and splits are performed recursively as we move from top to bottom.\nWhen it comes to analyzing data from social networks, hierarchical clustering is by far the most common and popular method of clustering. The nodes (branches) in the graph are compared to each other depending on the degree of similarity that exists between them. By linking together smaller groups of nodes that are related to one another, larger groupings may be created. The biggest advantage of hierarchical clustering is that it is easy to understand and implement.\nModel Based Clustering\nThese clustering models are based on the notion of how probable it is that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is the Expectation-maximization algorithm which uses multivariate normal distributions.\nDensity Based Clustering\nThese models search the data space for areas of the varied density of data points in the data space. They isolate different dense regions and assign the data points within these regions to the same cluster. Popular examples of density models are DBSCAN and OPTICS. These models are particularly useful for identifying clusters of arbitrary shape and detecting outliers, as they can detect and separate points that are located in sparse regions of the data space, as well as points that belong to dense regions.\nFuzzy Clustering\nFuzzy Clustering is a type of clustering algorithm in machine learning that allows a data point to belong to more than one cluster with different degrees of membership. Unlike traditional clustering algorithms, such as k-means or hierarchical clustering, which assign each data point to a single cluster, fuzzy clustering assigns a membership degree between 0 and 1 for each data point for each cluster.\nK-Mean Clustering of Iris Dataset\nNow, I am going to identify clusters of the iris dataset using the petal length and petal width. As a first step of identifying the number of clusters, I am going to calculate the inertia and determine the number of clusters for the dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load Iris dataset\niris = datasets.load_iris()\n\n# Petal width and length\nX = iris.data[:, 2:4]  \n\n# The range of k values to try\nk_values = range(1, 11)\n\n# Calculate inertia for each k value\ninertia_values = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\n# Plot the inertia values\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, inertia_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.show()\n\n\n\n\nHere, we can use k=3. For the next step, I am going to use k=3 for clustering the data-\n\n# Choose the number of clusters (you can adjust this based on your needs)\nk = 3\n\n# Apply k-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the results\nplt.figure(figsize=(8, 5))\n\n# Plot the data points with color-coded clusters\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}')\n\n# Plot the cluster centers\nplt.scatter(centers[:, 0], centers[:, 1], marker='X', s=200, c='red', label='Cluster Centers')\n\n# Set plot labels and title\nplt.xlabel('Petal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('K-means Clustering on Iris Dataset (Petal Width vs. Petal Length)')\n\n# Show legend\nplt.legend()\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html",
    "href": "posts/Anomaly Detection/index.html",
    "title": "Anomaly Detection Using Machine Learning",
    "section": "",
    "text": "Anomaly detection is the process of identifying data points, entities or events that fall outside the normal range. An anomaly is anything that deviates from what is standard or expected. In practice, anomaly detection is often used to detect suspicious events, unexpected opportunities or bad data buried in time series data. A suspicious event might indicate a network breach, fraud, crime, disease or faulty equipment. An unexpected opportunity could involve finding a store, product or salesperson that’s performing much better than others and should be investigated for insight into improving the business.\n\n\nThere are three types of anomalies-\n\nGlobal\nWhen a data point assumes a value that is far outside all the other data point value ranges in the dataset, it can be considered a global anomaly. In other words, it's a rare event. For example, if you receive an average American salary to your bank accounts each month but one day get a million dollars, that would look like a global anomaly to the bank's analytics team.\nContextual\nWhen an outlier is called contextual it means that its value doesn't correspond with what we expect to observe for a similar data point in the same context. Contexts are usually temporal, and the same situation observed at different times can be not an outlier. For example, for stores it's quite normal to experience an increase in customers during the holiday season. However, if a sudden boost happens outside of holidays or sales, it can be considered a contextual outlier.\nCollective\nCollective outliers are represented by a subset of data points that deviate from the normal behavior. In general, tech companies tend to grow bigger and bigger. Some companies may decay but it's not a general trend. However, if many companies at once show a decrease in revenue in the same period of time, we can identify a collective outlier.\n\n\n\n\nSupervised\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\nUnsupervised\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it's impossible to predict all the anomalies that can occur in the dataset.\nSemi-supervised\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model's predictions more accurate.\nLocal outlier factor (LOF)\nLocal outlier factor is probably the most common technique for anomaly detection. This algorithm is based on the concept of the local density. It compares the local density of an object with that of its neighbouring data points. If a data point has a lower density than its neighbours, then it is considered an outlier.\nK-nearest neighbors\nkNN is a supervised ML algorithm often used for classification. When applied to anomaly detection problems, kNN is a useful tool because it allows to easily visualize the data points on the scatterplot and make anomaly detection much more intuitive. Another benefit of kNN is that it works well on both small and large datasets.\nInstead of learning 'normal' and 'abnormal' values to solve the classification problem, kNN doesn't perform any actual learning. So when it comes to anomaly detection, kNN works as an unsupervised learning algorithm. A machine learning expert defines a range of normal and abnormal values manually, and the algorithm breaks this representation into classes by itself.\nSupport vector machines\nSupport vector machine (SVM) is also a supervised machine learning algorithm often used for classification. SVMs use hyperplanes in multi-dimensional space to divide data points into classes.\nSVM is usually applied when there are more than one classes involved in the problem. However, in anomaly detection it's also used for single class problems. The model is trained to learn the 'norm' and can identify whether unfamiliar data belongs to this class or represents an anomaly.\nDBSCAN\nThis is an unsupervised ML algorithm based on the principle of density. DBSCAN is able to uncover clusters in large spatial datasets by looking at the local density of the data points and generally shows good results when used for anomaly detection. The points that do not belong to any cluster get their own class: -1 so they are easy to identify. This algorithm handles outliers well when the data is represented by non-discrete data points.\nAutoencoders\nThis algorithm is based on the use of artificial neural networks that encode the data by compressing it into the lower dimensions. Then, ANNs decode the data to reconstruct the original input. When we reduce the dimensionality, we don't lose the necessary information because the rules have already been identified in the compressed data. Now we can already discover outliers.\nBayesian networks\nBayesian networks enable ML engineers to discover anomalies even in high-dimensional data. This method is used when the anomalies that we're looking for are more subtle and harder to discover and visualizing them on the plot might not produce the desired results.\nBelow is an example where I have used titanic dataset to detect anomalies-\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\n\ntitanic = pd.read_csv('titanic.csv')\n\n# Extract numerical features (you may choose other features based on your requirement)\nnumerical_features = titanic[['Age', 'Fare']]\n\n# Handling missing values (you may need to handle other missing values based on your dataset)\nnumerical_features = numerical_features.dropna()\n\n# Create and fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.01, random_state=42)\nmodel.fit(numerical_features)\n\n# Predict outliers (anomalies)\npredictions = model.predict(numerical_features)\ntitanic['prediction'] = predictions\n\n# Separate normal and anomaly points\nnormal_points = numerical_features[titanic['prediction'] == 1]\nanomaly_points = numerical_features[titanic['prediction'] == -1]\n\n# Visualize the results\nplt.scatter(normal_points['Age'], normal_points['Fare'], c='blue', label='Normal', alpha=0.7)\nplt.scatter(anomaly_points['Age'], anomaly_points['Fare'], c='red', label='Anomaly', alpha=0.7)\n\nplt.title('Isolation Forest Anomaly Detection on Titanic Dataset')\nplt.xlabel('Age')\nplt.ylabel('Fare')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#types-of-anomalies",
    "href": "posts/Anomaly Detection/index.html#types-of-anomalies",
    "title": "Anomaly Detection Using Machine Learning",
    "section": "",
    "text": "There are three types of anomalies-\n\nGlobal\nWhen a data point assumes a value that is far outside all the other data point value ranges in the dataset, it can be considered a global anomaly. In other words, it's a rare event. For example, if you receive an average American salary to your bank accounts each month but one day get a million dollars, that would look like a global anomaly to the bank's analytics team.\nContextual\nWhen an outlier is called contextual it means that its value doesn't correspond with what we expect to observe for a similar data point in the same context. Contexts are usually temporal, and the same situation observed at different times can be not an outlier. For example, for stores it's quite normal to experience an increase in customers during the holiday season. However, if a sudden boost happens outside of holidays or sales, it can be considered a contextual outlier.\nCollective\nCollective outliers are represented by a subset of data points that deviate from the normal behavior. In general, tech companies tend to grow bigger and bigger. Some companies may decay but it's not a general trend. However, if many companies at once show a decrease in revenue in the same period of time, we can identify a collective outlier."
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#methods-of-anomaly-detection",
    "href": "posts/Anomaly Detection/index.html#methods-of-anomaly-detection",
    "title": "Anomaly Detection Using Machine Learning",
    "section": "",
    "text": "Supervised\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\nUnsupervised\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it's impossible to predict all the anomalies that can occur in the dataset.\nSemi-supervised\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model's predictions more accurate.\nLocal outlier factor (LOF)\nLocal outlier factor is probably the most common technique for anomaly detection. This algorithm is based on the concept of the local density. It compares the local density of an object with that of its neighbouring data points. If a data point has a lower density than its neighbours, then it is considered an outlier.\nK-nearest neighbors\nkNN is a supervised ML algorithm often used for classification. When applied to anomaly detection problems, kNN is a useful tool because it allows to easily visualize the data points on the scatterplot and make anomaly detection much more intuitive. Another benefit of kNN is that it works well on both small and large datasets.\nInstead of learning 'normal' and 'abnormal' values to solve the classification problem, kNN doesn't perform any actual learning. So when it comes to anomaly detection, kNN works as an unsupervised learning algorithm. A machine learning expert defines a range of normal and abnormal values manually, and the algorithm breaks this representation into classes by itself.\nSupport vector machines\nSupport vector machine (SVM) is also a supervised machine learning algorithm often used for classification. SVMs use hyperplanes in multi-dimensional space to divide data points into classes.\nSVM is usually applied when there are more than one classes involved in the problem. However, in anomaly detection it's also used for single class problems. The model is trained to learn the 'norm' and can identify whether unfamiliar data belongs to this class or represents an anomaly.\nDBSCAN\nThis is an unsupervised ML algorithm based on the principle of density. DBSCAN is able to uncover clusters in large spatial datasets by looking at the local density of the data points and generally shows good results when used for anomaly detection. The points that do not belong to any cluster get their own class: -1 so they are easy to identify. This algorithm handles outliers well when the data is represented by non-discrete data points.\nAutoencoders\nThis algorithm is based on the use of artificial neural networks that encode the data by compressing it into the lower dimensions. Then, ANNs decode the data to reconstruct the original input. When we reduce the dimensionality, we don't lose the necessary information because the rules have already been identified in the compressed data. Now we can already discover outliers.\nBayesian networks\nBayesian networks enable ML engineers to discover anomalies even in high-dimensional data. This method is used when the anomalies that we're looking for are more subtle and harder to discover and visualizing them on the plot might not produce the desired results.\nBelow is an example where I have used titanic dataset to detect anomalies-\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\n\ntitanic = pd.read_csv('titanic.csv')\n\n# Extract numerical features (you may choose other features based on your requirement)\nnumerical_features = titanic[['Age', 'Fare']]\n\n# Handling missing values (you may need to handle other missing values based on your dataset)\nnumerical_features = numerical_features.dropna()\n\n# Create and fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.01, random_state=42)\nmodel.fit(numerical_features)\n\n# Predict outliers (anomalies)\npredictions = model.predict(numerical_features)\ntitanic['prediction'] = predictions\n\n# Separate normal and anomaly points\nnormal_points = numerical_features[titanic['prediction'] == 1]\nanomaly_points = numerical_features[titanic['prediction'] == -1]\n\n# Visualize the results\nplt.scatter(normal_points['Age'], normal_points['Fare'], c='blue', label='Normal', alpha=0.7)\nplt.scatter(anomaly_points['Age'], anomaly_points['Fare'], c='red', label='Anomaly', alpha=0.7)\n\nplt.title('Isolation Forest Anomaly Detection on Titanic Dataset')\nplt.xlabel('Age')\nplt.ylabel('Fare')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Anomaly Detection Using Machine Learning\n\n\n\n\n\n\n\nMachine Learning\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nClassification in Machine Learning\n\n\n\n\n\n\n\nMachine Learning\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-linear Regression Model\n\n\n\n\n\n\n\nMachine Learning\n\n\nRegression Model\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variable\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nMd Shazalal Tushar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification in Machine Learning",
    "section": "",
    "text": "Classification\nA classification algorithm is a machine learning algorithm that categorizes or assigns predefined labels or classes to data based on its features or attributes. It is a Supervised Learning Technique used to classify new observations.\nThe Classification algorithm uses labeled input data since it is a supervised learning technique that includes input and output data. The classification procedure (x) converts a discrete output function (y) to an input variable.\nNumerous issues can be solved using classification algorithms, such as image recognition, sentiment analysis, medical diagnosis, and spam email detection. The classification algorithm to be used is determined by the nature of the data as well as the specific requirements of the problem at hand, as different algorithms may perform better for different types of data and tasks.\nThere are two types of learners in machine learning classification: lazy and eager learners.\nEager learners are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time during the training process because of their eagerness to have a better generalization during the training from learning the weights, but they require less time to make predictions.\nMost machine learning algorithms are eager learners, and below are some examples:\n\nLogistic Regression.\nSupport Vector Machine.\nDecision Trees.\nArtificial Neural Networks.\n\nLazy learners or instance-based learners, on the other hand, do not create any model immediately from the training data, and this is where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they search for the nearest neighbor from the whole training data, which makes them very slow during prediction. Some examples of this kind are:\n\nK-Nearest Neighbor.\nCase-based reasoning.\n\nHowever, some algorithms, such asBallTrees andKDTrees, can be used to improve the prediction latency.\nDifferent Types of Classification Tasks in Machine Learning\nThere are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications.\nBinary Classification\nIn a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled.Logistic Regression and Support Vector Machines algorithms are natively designed for binary classifications. However, other algorithms such as K-Nearest Neighbors and Decision Trees can also be used for binary classification.\nMulti-class Classification\nThe multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. Most of the binary classification algorithms can be also used for multi-class classification. These algorithms include- Random Forest, Naive Bayes, K-Nearest Neighbors, Gradient Boosting, SVM, Logistic Regression.\nMulti-label Classification\nIn multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label. It is not possible to use multi-class or binary classification models to perform multi-label classification. However, most algorithms used for those standard classification tasks have their specialized versions for multi-label classification such as- Multi-label Decision Trees, Multi-label Gradient Boosting, and Multi-label Random Forests\nImbalance Classification\nFor the imbalanced classification, the number of examples is unevenly distributed in each class, meaning that we can have more of one class than the others in the training data.Using conventional predictive models such as Decision Trees, Logistic Regression, etc. could not be effective when dealing with an imbalanced dataset, because they might be biased toward predicting the class with the highest number of observations, and considering those with fewer numbers as noise. The most commonly used approaches include sampling techniques or harnessing the power of cost-sensitive algorithms.\nTypes of Classification in Machine Learning\nThere are seven types of classification in machine learning and all seven models are called deep learning classification models.\nLogistic Regression\nIn this algorithmic classification, using logistic functions, the possible outcome of a single trial is modelled. The advantage of this logistic regression is that it receive multiple variables and gives a single output variable. It works when a binary classification machine learningvariable is present.\nNaive Bayes\nBayes is the theorem of algorithmic classification for every single feature. Classification and spam filtering work in many real-world documents. Getting the necessary parameters requires a small amount of training and works extremely fast compared to more experienced methods. It is the advantage of naive Bayes. It works only when there is a predictor variable. And this is the disadvantage of Naive Bayes.\nStochastic Gradient Descent\nIn linear models of algorithm classification, stochastic gradient descent works very easily and efficiently, supporting the function and penalties. It is structured and simple to execute. This is the advantage of stochastic gradient descent. It is hard to scale. Hence, it requires hyper-parameters. This is the disadvantage of stochastic gradient descent.\nK-Nearest Neighbors\nNeighbour’s algorithm classification is known as lazy learning. It does not work in a general internal model but simply stores the training data. It has a simple majority vote for each point. Neighbor algorithm classification is easy to implement and contains a large number of training datasets. This is the advantage of having neighbors. The K value is high and needs to be controlled. This is the disadvantage of the neighbor’s classification.\nDecision Tree\nThe classes get the attribute of datasets to classify. The decision tree can handle both numerical and categorical datasets in algorithmic classification. It is easy to understand and visualize. This is the advantage of the decision tree. If it is not generalized well, it may create a decision-tree complex. This is the disadvantage of the decision tree algorithm classification.\nRandom Forest\nFor overfitting a model and controlling the Meta, the estimator takes the number of various decision trees to improve the classifier in a random forest. Overfitting and the random forest are better classifiers. It is the advantage of a random forest. It has a complement algorithm for classification and is difficult to implement. And this is the disadvantage of random forests.\nSupport vector Machine\nSupport vector machine takes the training data as points and spaces them out into categories by clearing the gap in this algorithm’s classification. It is high-dimensional and memory-efficient. This is the advantage of a support vector machine. The algorithmic classification is not provided directly, and they are very expensive in five-fold cross-validation. And this is the disadvantage of a support vector machine.\nBelow is an example where I have used the iris dataset to predict the species using linear regression model\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# Load the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Create a DataFrame for better visualization\nconf_df = pd.DataFrame(conf_matrix, index=iris.target_names, columns=iris.target_names)\n\n# Specify the plot size\nplt.figure(figsize=(8, 8))\n\n# Create a heatmap using Matplotlib's imshow\nplt.imshow(conf_df, interpolation='nearest', cmap=plt.cm.Greens)\n\n# Add annotations with actual values\nfor i in range(len(iris.target_names)):\n    for j in range(len(iris.target_names)):\n        plt.text(j, i, str(conf_df.iloc[i, j]), ha='center', va='center')\n\nplt.title('Confusion Matrix Heatmap')\nplt.colorbar()\nplt.xticks(np.arange(len(iris.target_names)), iris.target_names, rotation=0)\nplt.yticks(np.arange(len(iris.target_names)), iris.target_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nC:\\Users\\shaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nNow, I will calculate the accuracy of the model-\n\n# Calculating the accuracy of the classification\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of the model is {:.2f}\".format(accuracy))\n\nAccuracy of the model is 97.37\n\n\nThe accuracy of the model is 97.37% as it predicted one virginica which is versicolor in reality."
  },
  {
    "objectID": "posts/Linear and Non-linear Regression Model/index.html",
    "href": "posts/Linear and Non-linear Regression Model/index.html",
    "title": "Linear and Non-linear Regression Model",
    "section": "",
    "text": "Linear Regression is a supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable. In other words, it predicts the output variables based on the independent input variable.\nThere are two types of linear regression models- Simple and Multiple.\nSimple Linear Regression\nIn the simple regression model, there is only one independent variable and the model finds the linear relationship of it with the dependent variable. The equation of a Simple Linear Regression Model is-\n\\[ y= \\beta_0 + \\beta_1 x+ \\epsilon_0 \\]\nHere, y is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the regression coefficient of the independent variable x, \\(\\epsilon_0\\) is the error term of the regression model.\nMultiple Linear Regression\nThe difference between simple linear regression and multiple regression model is that multiple regression model has more than one independent variables. The equation of a multiple linear regression model is given below-\n\n\\[ y = \\beta_0+ \\beta_1* x_1+ \\beta_2 * x_2+ \\beta_3 * x_3+.............+\\beta_n * x_n+ \\epsilon \\]\nHere, y is the dependent or target variable, \\(\\beta_0\\) is the intercept of the regression line in y axis, \\(\\beta_1,\\beta_2,\\beta_3, \\beta_n\\) are the regression co-efficients of independent variables \\(x_1,x_2, x_3, x_n\\). \\(\\epsilon\\) indicates the error term of the regression model.\nA linear regression model’s main aim is to find the best fit linear line and the optimal values of intercept and coefficients such that the error is minimized. Error is the difference between the actual value and predicted value and the goal is to reduce the difference.\nAssumption of Linear Regression\nThere are six assumption of linear regression-\n\nLinearity: It states that the dependent variable Y should be linearly related to independent variables. This assumption can be checked by plotting a scatter plot between both variables.\nNormality: The X and Y variables should be normally distributed. Histograms, KDE plots, Q-Q plots can be used to check the Normality assumption.\nHomoscedasticity: The variance of the error terms should be constant i.e. the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\nIndependent/ No Multicollinearity: The variables should be independent of each other i.e. no correlation should be there between the independent variables. To check this assumption, we can use correlation matrix or VIF score. If the VIF score is less than 5 then there is no significant correlation present.\nThe error terms should be normally distributed. Q-Q plots and Histograms can be used to check the distribution of error terms.\nNo Autocorrelation: The error terms should be independent of each other. Autocorrelation can be tested using the Durbin Watson test. The null hypothesis assumes that there is no autocorrelation. The value of the test lies between 0 to 4. If the value of the test is 2 then there is no autocorrelation.\n\nThe following is an example of Multiple Linear Regression Model of King County, Washington housing data. For the first step, I will identify the correlation between the independent variables while the dependent variable is the housing price. As a first step, I will remove the cells with no values.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nData= pd.read_csv(\"kc_house_data.csv\")\nData.columns\nData.dropna(inplace=True)\n\nTo keep the model simple, I will use only three variables- Number of bedrooms, Number of bathrooms, and square footage of the living room to predict the price of housing. Hence, I will remove all the other columns from the dataframe.\n\ndrop_columns= ['id', 'date','sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode','lat', 'long', 'sqft_living15', 'sqft_lot15']\nData.drop(columns=drop_columns,inplace=True)\nData.head(5)\n\n\n\n\n\n\n\n\nprice\nbedrooms\nbathrooms\nsqft_living\n\n\n\n\n0\n221900.0\n3\n1.00\n1180\n\n\n1\n538000.0\n3\n2.25\n2570\n\n\n2\n180000.0\n2\n1.00\n770\n\n\n3\n604000.0\n4\n3.00\n1960\n\n\n4\n510000.0\n3\n2.00\n1680\n\n\n\n\n\n\n\nNow, I will divide the dataset into training set (70%) and test set (30%). Based on the training set, I will develop the regression model and later predict the housing price based on the model.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n# List of independent variables\nindependent_var = [\"bedrooms\", \"bathrooms\", \"sqft_living\"]\n\n# Extract independent and dependent variables\nX = Data[independent_var]\ny = Data['price']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model performance\ndef performance_stat(y_test, y_pred):\n  from sklearn.metrics import r2_score\n\n  # Calculate regression metrics\n  r2 = r2_score(y_test, y_pred)\n  print(f'R-squared (R²): {r2:.2f}')\n\n# Call the performance_stat function\nperformance_stat(y_test, y_pred)\n\nR-squared (R²): 0.52\n\n\nHere, the \\(R^2\\) value of 0.52 indicates that about 52% of variability of the housing price can be explained by the three independent variables. Now, I will generate a scatterplot to see the model performance in terms of predicting the actual housing price.\n\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_pred, y_test, color='green', alpha=0.1)\n\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='red', linewidth=2, label='1:1 Line')\n\nplt.xlabel('Predicted Housing Price')\nplt.ylabel('Actual Housing Price')\nplt.title('Linear Regression on King County Housing Dataset')\nplt.legend()\nplt.show()\n\n\n\n\nNon-Linear Regression Model\nNon-linear regression algorithms are machine learning techniques used to model and predict non-linear relationships between input variables and target variables. These algorithms aim to capture complex patterns and interactions that cannot be effectively represented by a linear model. Here are some popular non-linear regression algorithms. The example of non-linear regression models are-\n\nDecision Trees\nRandom Forest\nSupport Vector Regression (SVR)\nK-Nearest Neighbors (KNN)\nArtificial Neural Networks (ANN)\nGradient Boosting\nPolynomial Regression\nAdaBoost Regression\nExtra Trees Regression\nBayesian Ridge Regression\nKernel Ridge Regression\n\nApplying Random Forest to the housing dataset\nNow, I will use Random Forest to predict the housing price based on the number of bedrooms, bathrooms, and square footage of the living room and check which model performed better based on the \\(R^2\\) value.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create and train the Random Forest Regression model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_rf = model.predict(X_test)\n\n# Evaluate the model\nr2_rf = r2_score(y_test, y_pred)\n\nprint(f'R-squared of RF (R²): {r2_rf:.2f}')\n\nR-squared of RF (R²): 0.52\n\n\nHere, the \\(R^2\\) value of the Random Forest regression model is also 52% indicating both of the models performed similarly.\n\n# Visualize the results (2D scatter plot for simplicity)\nplt.figure(figsize=(8, 8))\nplt.scatter(y_pred_rf, y_test, color='grey', alpha= 0.1)\n# Plot a 45 degree line for reference\nplt.plot([0, y_test.max()], [0, y_test.max()], linestyle='--', color='black', linewidth=2, label='1:1 Line')\nplt.xlabel('Predicted housing price in RF')\nplt.ylabel('Actual housing price')\nplt.legend()\nplt.show()"
  }
]